{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U chromadb"
      ],
      "metadata": {
        "id": "Kw3v_r9cVhrf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "from openai import OpenAI\n",
        "from tiktoken import encoding_for_model\n",
        "from pydantic import BaseModel\n",
        "import json"
      ],
      "metadata": {
        "id": "mkRGblRBVIGl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path setting\n",
        "dir_path = '/content/drive/MyDrive/Colab_Notebooks/'\n",
        "\n",
        "# API keys\n",
        "openai_file = \"googlecolab_openai_key.txt\"\n",
        "with open(dir_path + openai_file, \"r\") as file:\n",
        "    openai_api_key = file.read()\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "yiw-PnpOVm6H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI client and Chorma client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "chroma_client = chromadb.PersistentClient(path=dir_path+\"+repo_search/chroma_db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAnkQi4rVoP8",
        "outputId": "dcca5ecb-81f6-44b7-89ce-48305af37271"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chroma_client.delete_collection(\"example_collection\")"
      ],
      "metadata": {
        "id": "Y8bbbGi5VvHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding model\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction() # by default, all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "Yyn32uAAVuH8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information on repo to search.\n",
        "# In the function 'get_contents', need the info for api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "owner=\"woo1976\"\n",
        "repo=\"jenkins-python-test\"\n",
        "path=\"\"\n",
        "branch=\"master\""
      ],
      "metadata": {
        "id": "FWtcU5VQV2Wc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "def get_repo_info(repo_url):\n",
        "    \"\"\"\n",
        "    Extract the owner and repository name from the GitHub URL.\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(repo_url)\n",
        "    path_parts = parsed_url.path.strip('/').split('/')\n",
        "    if len(path_parts) < 2:\n",
        "        raise ValueError(\"Invalid GitHub repository URL.\")\n",
        "    owner = path_parts[0]\n",
        "    repo = path_parts[1].replace('.git', '')\n",
        "    return owner, repo\n",
        "\n",
        "def get_file_content(download_url):\n",
        "    \"\"\"\n",
        "    Retrieve the raw content of a file.\n",
        "    \"\"\"\n",
        "    response = requests.get(download_url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "def get_contents(owner=\"woo1976\", repo=\"jenkins-python-test\", path=\"\", branch=\"master\"):\n",
        "    \"\"\"\n",
        "    Recursively retrieve the contents of the repository, excluding image and media files.\n",
        "    \"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "    params = {\"ref\": branch}  # You can change the branch if needed\n",
        "    response = requests.get(api_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    items = response.json()\n",
        "    contents = []\n",
        "\n",
        "    if not isinstance(items, list):\n",
        "        items = [items]\n",
        "\n",
        "    # Exclude image and media formats\n",
        "    excluded_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.mp4', '.mp3', '.wav', '.avi', '.mov']\n",
        "\n",
        "    for item in items:\n",
        "        if item['type'] == 'file' and any(item['path'].endswith(ext) for ext in excluded_extensions):\n",
        "            continue\n",
        "\n",
        "        if item['type'] == 'file':\n",
        "            file_content = get_file_content(item['download_url'])\n",
        "            contents.append({\n",
        "                'type': 'file',\n",
        "                'path': item['path'],\n",
        "                'content': file_content\n",
        "            })\n",
        "        elif item['type'] == 'dir':\n",
        "            dir_contents = get_contents(owner, repo, item['path'], branch)\n",
        "            contents.append({\n",
        "                'type': 'dir',\n",
        "                'path': item['path'],\n",
        "                'contents': dir_contents\n",
        "            })\n",
        "    return contents\n",
        "\n",
        "def flatten_contents(contents):\n",
        "    \"\"\"\n",
        "    Flatten the nested contents into a list of files with paths and contents.\n",
        "    \"\"\"\n",
        "    flat_files = []\n",
        "\n",
        "    def _flatten(items):\n",
        "        for item in items:\n",
        "            if item['type'] == 'file':\n",
        "                flat_files.append({\n",
        "                    'path': item['path'],\n",
        "                    'content': item['content']\n",
        "                })\n",
        "            elif item['type'] == 'dir':\n",
        "                _flatten(item['contents'])\n",
        "\n",
        "    _flatten(contents)\n",
        "    return flat_files\n",
        "\n",
        "def truncate_to_token_limit(text, max_tokens, encoding='gpt-4o'):\n",
        "    \"\"\"\n",
        "    Truncate the text to fit within the specified token limit.\n",
        "    \"\"\"\n",
        "    tokenizer = encoding_for_model(encoding)\n",
        "    tokens = tokenizer.encode(text)\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "    truncated_tokens = tokens[:max_tokens]\n",
        "    return tokenizer.decode(truncated_tokens)\n",
        "\n",
        "def summarize_text(text, max_tokens=200):\n",
        "    \"\"\"\n",
        "    Summarize the text to fit within a limited number of tokens.\n",
        "    \"\"\"\n",
        "    prompt = f\"Summarize the following text to {max_tokens} tokens:\\n\\n{text}\\n\\nSummary:\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generate_gpt4o_response(query, context_docs, max_context_tokens=6000):\n",
        "    \"\"\"\n",
        "    Generate a response using OpenAI's GPT-4o based on the query and retrieved context documents.\n",
        "    \"\"\"\n",
        "    # Summarize to reduce token size\n",
        "    summarized_contexts = [summarize_text(doc, max_tokens=200) for doc in context_docs]\n",
        "\n",
        "    # Combine into a single string\n",
        "    combined_context = \"\\n\\n\".join(summarized_contexts)\n",
        "\n",
        "    # Truncate\n",
        "    context = truncate_to_token_limit(combined_context, max_context_tokens)\n",
        "\n",
        "    prompt = f\"\"\"You are an assistant that provides detailed answers based on the following context.\n",
        "Please generate 3 distinct answers in the order from the most relevant one to the least relevant one.\n",
        "Each answer should be different in terms of file names and contents.\n",
        "Each answer should include a file name and its path.\n",
        "It would be also better for each asnwer to include summary of the file\n",
        "Lastly, it would be best to have the most relevant code snippet from the script:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "       response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.2\n",
        "        )\n",
        "       answer = response.choices[0].message.content.strip()\n",
        "\n",
        "       return answer\n",
        "        # Save the below for later usage just in case.\n",
        "        # response = client.beta.chat.completions.parse(\n",
        "        #     model=\"gpt-4o\",\n",
        "        #     messages=[\n",
        "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        #         {\"role\": \"user\", \"content\": prompt}\n",
        "        #     ],\n",
        "        #     max_tokens=1000,\n",
        "        #     temperature=0.2,\n",
        "        #     response_format=ResFormat,\n",
        "        # )\n",
        "        # answer = response.choices[0].message.content\n",
        "        # json_answer = json.loads(answer)\n",
        "\n",
        "        # return json_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating GPT-4o response: {e}\")\n",
        "        return \"I'm sorry, I couldn't process your request at the moment.\""
      ],
      "metadata": {
        "id": "3N5WiOueV47m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a structured output for LLM results (save this for later usage just in case)\n",
        "# class ResFormat(BaseModel):\n",
        "#     file_name: str\n",
        "#     file_path: str\n",
        "#     summary: str\n",
        "#     code_snippet: str\n",
        "\n",
        "#  Extract repo contents\n",
        "res = get_contents(owner=owner, repo=repo, path=path, branch=branch)\n",
        "flat_files = flatten_contents(res)\n",
        "\n",
        "texts = [file['content'] for file in flat_files]\n",
        "metadatas = [{'path': file['path']} for file in flat_files]\n",
        "ids = [file['path'] for file in flat_files]\n",
        "\n",
        "# Create vector DB\n",
        "collection = chroma_client.get_or_create_collection(name=\"example_collection\", embedding_function=default_ef)\n",
        "collection.add(documents=texts, metadatas=metadatas, ids=ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQXJv70SV-bQ",
        "outputId": "2c12fa2e-7e9c-438c-d93d-68bccf6f61f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:01<00:00, 70.8MiB/s]\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query results from vector DB\n",
        "query = \"Give me a script that is related to testing.\"\n",
        "top_k = 5\n",
        "results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "retrieved_docs_with_metadata = []\n",
        "for i in range(len(results['documents'][0])):\n",
        "  doc = results['documents'][0][i]\n",
        "  metadata = results['metadatas'][0][i]\n",
        "  retrieved_docs_with_metadata.append({'document': doc, 'metadata': metadata})\n",
        "context_docs = [doc for doc in retrieved_docs_with_metadata]"
      ],
      "metadata": {
        "id": "q0c0_1cXWBQu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate LLM agent answers\n",
        "answer = generate_gpt4o_response(query, context_docs)"
      ],
      "metadata": {
        "id": "a-8-QABEWJbf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)\n",
        "\n",
        "# Save the below for later usage just in case.\n",
        "# print(answer['file_name'])\n",
        "# print(answer['file_path'])\n",
        "# print(answer['summary'])\n",
        "# print(answer['code_snippet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4alt2c9WLh9",
        "outputId": "0fbe234f-de20-41ce-c6dc-60c9983ab3a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **File Name:** `test_iris.py`  \n",
            "   **Path:** `tests/test_iris.py`  \n",
            "   **Summary:** This Python script is a test file using the pytest framework and the Click library to test command-line interfaces. It defines a test class `TestCLI` with a pytest fixture `runner` that returns a `CliRunner` instance for simulating CLI interactions. The script includes a single test method, `test_print_help_succeeds`, which checks if the `iris.cli` command with the `--help` option executes successfully.  \n",
            "   **Code Snippet:**  \n",
            "   ```python\n",
            "   import pytest\n",
            "   from click.testing import CliRunner\n",
            "   from iris import cli\n",
            "\n",
            "   class TestCLI:\n",
            "       @pytest.fixture\n",
            "       def runner(self):\n",
            "           return CliRunner()\n",
            "\n",
            "       def test_print_help_succeeds(self, runner):\n",
            "           result = runner.invoke(cli, ['--help'])\n",
            "           assert result.exit_code == 0\n",
            "   ```\n",
            "\n",
            "2. **File Name:** `unit_tests.xml`  \n",
            "   **Path:** `reports/unit_tests.xml`  \n",
            "   **Summary:** This XML document is a test suite report generated by pytest, summarizing the results of a unit test execution. It shows that there were no errors, failures, or skipped tests, and it includes a single test case named \"test_print_help_succeeds\" from the class \"tests.test_iris.TestCLI\". The test suite, named \"pytest\", took a total of 0.286 seconds to run, with the specific test case executing successfully in 0.0015 seconds.  \n",
            "   **Code Snippet:**  \n",
            "   ```xml\n",
            "   <testsuite name=\"pytest\" tests=\"1\" errors=\"0\" failures=\"0\" skipped=\"0\" time=\"0.286\">\n",
            "       <testcase classname=\"tests.test_iris.TestCLI\" name=\"test_print_help_succeeds\" time=\"0.0015\"/>\n",
            "   </testsuite>\n",
            "   ```\n",
            "\n",
            "3. **File Name:** `Jenkinsfile`  \n",
            "   **Path:** `Jenkinsfile`  \n",
            "   **Summary:** This Jenkins pipeline script automates the software development process, including a stage for executing unit tests with Pytest. The pipeline is configured to trigger every five minutes on weekdays and includes stages for checking the PATH, pulling code, setting up the environment, generating static code metrics, running unit tests, and more. The unit test stage archives the results, which are likely generated by pytest.  \n",
            "   **Code Snippet:**  \n",
            "   ```groovy\n",
            "   stage('Unit Tests') {\n",
            "       steps {\n",
            "           script {\n",
            "               sh 'pytest --junitxml=reports/unit_tests.xml'\n",
            "           }\n",
            "           archiveArtifacts artifacts: 'reports/unit_tests.xml', allowEmptyArchive: true\n",
            "       }\n",
            "   }\n",
            "   ```\n"
          ]
        }
      ]
    }
  ]
}